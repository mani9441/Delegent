import os
import requests
import warnings
from typing import Optional, List
from dotenv import load_dotenv
from tenacity import retry, stop_after_attempt, wait_fixed, retry_if_exception_type

from langchain_core.language_models.llms import LLM
from langchain_core.language_models.chat_models import BaseChatModel
from langchain_core.messages import AIMessage, BaseMessage
from langchain_core.outputs import ChatGeneration, ChatResult

# Suppress warnings
warnings.filterwarnings("ignore")

# Load environment variables
load_dotenv()

class LocalOllamaLLM(BaseChatModel):
    """
    LLM class for interacting with a local Ollama instance via HTTP API.

    Requires these environment variables:
    - LOCAL_OLLAMA_API_ENDPOINT (default: http://localhost:11434/api/generate)
    - LOCAL_OLLAMA_MODEL_NAME   (default: llama3)
    """

    endpoint: str = os.getenv("LOCAL_OLLAMA_API_ENDPOINT", "http://localhost:11434/api/generate")
    model: str = os.getenv("LOCAL_OLLAMA_MODEL_NAME", "llama3")

    @retry(
        stop=stop_after_attempt(3),
        wait=wait_fixed(2),
        retry=retry_if_exception_type(requests.RequestException)
    )
    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:
        """
        Sends a prompt to the local Ollama server and returns the generated response.

        Args:
            prompt (str): The input prompt.
            stop (Optional[List[str]]): Optional stop sequences (ignored in local API).

        Returns:
            str: The response generated by the model.

        Raises:
            ValueError: For network issues or invalid responses.
        """
        payload = {
            "model": self.model,
            "prompt": prompt,
            "stream": False
        }

        try:
            response = requests.post(self.endpoint, json=payload, timeout=40)
            response.raise_for_status()

            data = response.json()
            generated_text = data.get("response")

            if not generated_text:
                raise ValueError("No 'response' field found in Ollama response.")

            return generated_text

        except requests.RequestException as e:
            raise ValueError(f"[LocalOllamaLLM] Connection error: {e}")
        except Exception as e:
            raise ValueError(f"[LocalOllamaLLM] Unexpected error: {e}")

    def _generate(self, messages: List[BaseMessage], stop: Optional[List[str]] = None) -> ChatResult:
        prompt = "\n".join([str(msg.content) for msg in messages])  # <-- fix here
        response_text = self._call(prompt, stop)  # your original call method
        return ChatResult(
            generations=[ChatGeneration(message=AIMessage(content=response_text))]
        )

    @property
    def _llm_type(self) -> str:
        """Returns the identifier for LangChain LLM."""
        return "local_ollama"
